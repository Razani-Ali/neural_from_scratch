# Neural Networks from Scratch

This repository contains neural network implementations written from scratch in Python and numpy.

## Features
- Activation functions: ReLU, Sigmoid, Tanh, Elu, Leaky_ReLu, Softmax
- Dense, RBF, and rough networks
- RNN layers like Elman, Jordan, TimeDistributer
- Custom loss functions and weight initialization
- Live and offline visualizations
- Preprocessing scripts for data preparation
- Jupyter notebook demonstrating model performance

## Installation
To run this project, install the required dependencies:
```bash
pip install -r requirements.txt
```

## Google Colab Demonstrations
You can explore specific functionalities of this repository using the following Google Colab notebooks:
- [Levenberg Training](https://colab.research.google.com/drive/1EclN_iIodH_tFkfv_35GdiPTTwQJ0_DR?usp=sharing): Demonstrates training using Levenberg-Marquardt optimization.
- [Simple RNNs](https://colab.research.google.com/drive/16kicYcQdLPAB-98PFJ9swtwYDxHXcZUT?usp=sharing): Showcases implementations of simple RNN architectures.
- [Dense Layers](https://colab.research.google.com/drive/1CU8Sp2xoJd2PMx7rWItx9C6FulkwuEHl?usp=sharing): Explores Dense layer implementations.

## Usage
1. Clone the repository:
```bash
git clone https://github.com/Razani-Ali/neural_from_scratch.git
```
2. Navigate to the project directory:
```bash
cd neural_from_scratch
```
3. Start experimenting with the Jupyter notebooks or scripts.

## Contributing
Contributions are welcome! Feel free to open issues or submit pull requests.


